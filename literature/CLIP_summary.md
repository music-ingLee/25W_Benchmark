# CLIP (Contrastive Language-Image Pre-training) 정리

> 이 문서는 CLIP의 핵심 개념을 기초부터 차근차근 설명합니다.

---

## 1. CLIP이란?

**CLIP은 이미지와 텍스트를 "비교 가능하게" 만드는 AI 모델이다.**

OpenAI가 2021년에 발표했으며, 이미지와 텍스트를 같은 공간에 배치하여 서로 매칭할 수 있게 한다.

### 핵심 아이디어

```
이미지 → [이미지 인코더] → 512차원 벡터
텍스트 → [텍스트 인코더] → 512차원 벡터
                              ↓
                         유사도 비교 가능!
```

원래 이미지(픽셀)와 텍스트(글자)는 전혀 다른 형태라 비교가 불가능하다. CLIP은 둘 다 **512개의 숫자 목록(벡터)**으로 변환해서 비교를 가능하게 만든다.

---

## 2. 기초 개념

### 2.1 표상(Representation)이란?

**표상 = 무언가를 다른 형태로 "나타낸 것"**

| 원본 | 표상 |
|------|------|
| 실제 사과 | 머릿속 사과 이미지 |
| 서울 | 지도 위의 점 |
| 온도 30도 | 온도계의 빨간 막대 높이 |
| 고양이 사진 | CLIP의 512차원 벡터 |

뇌에서의 표상: 뇌는 세상의 것들을 **뉴런의 활동 패턴**으로 나타낸다. 이것이 "신경 표상(neural representation)"이다.

### 2.2 피처(Feature)란?

**피처 = 무언가를 설명하는 특징들**

바나나를 설명한다면: "길쭉함", "노란색", "휘어있음", "달콤함"

컴퓨터에서 피처는 **숫자 목록**으로 표현된다:
```
바나나 사진 → [0.9, 0.1, 0.8, 0.7, ...]
               ↑     ↑     ↑     ↑
           (각 숫자가 어떤 특징을 나타냄)
```

### 2.3 왜 512차원인가?

**512라는 숫자에 특별한 의미는 없다.** 연구자들이 실험적으로 정한 것이다.

선택 이유:
1. **2의 거듭제곱** (2⁹ = 512): 컴퓨터 계산에 효율적
2. **적당한 크기**: 너무 작으면 정보 부족, 너무 크면 계산 비용 과다

CLIP 버전별 차원:
| CLIP 버전 | 피처 차원 |
|-----------|----------|
| CLIP ViT-B/32 | 512 |
| CLIP ViT-L/14 | 768 |
| CLIP ViT-G/14 | 1280 |

---

## 3. 피처 추출: 어떻게 512개 숫자를 뽑는가?

### 3.1 핵심: 신경망이 스스로 배운다

사람이 "첫 번째 숫자는 노란색 정도"라고 정하는 것이 **아니다**.

4억 개의 (이미지, 텍스트) 쌍을 보여주면, 신경망이 알아서 **이미지를 숫자로 바꾸는 방법**을 찾아낸다.

### 3.2 각 피처의 의미를 알 수 있는가?

**솔직한 답: 정확히는 모른다.** 이것이 딥러닝의 "블랙박스 문제"이다.

하지만 알아내려는 방법들이 있다:

| 방법 | 설명 |
|------|------|
| 최대 활성화 이미지 | 특정 피처를 가장 크게 만드는 이미지들을 모아본다 |
| Probing | 특정 피처 값으로 어떤 속성을 예측할 수 있는지 테스트 |
| Ablation | 특정 피처를 0으로 만들면 어떤 능력이 사라지는지 관찰 |

**예시**: 피처 #247을 최대로 만드는 이미지들이 모두 털복숭이 동물이라면, 그 피처는 "털복숭이 동물"과 관련있다고 추론할 수 있다.

---

## 4. CLIP의 학습 과정

### 4.1 데이터 수집

인터넷에서 **이미지-텍스트 쌍 4억 개**를 수집한다.

```
쌍 #1: [고양이 사진] + "우리집 고양이"
쌍 #2: [비행기 사진] + "제주도 가는 비행기"
쌍 #3: [피자 사진] + "오늘 저녁 먹은 피자"
...
쌍 #4억: [바다 사진] + "여름휴가 바다"
```

원래 인터넷에 붙어있던 캡션(블로그, SNS, 뉴스 등)을 그대로 활용한다.

### 4.2 배치 단위 학습

4억 × 4억 연산은 불가능하다. 대신 **배치(batch) 단위**로 학습한다.

```
배치 크기 = 32,768개

한 번의 학습:
- 이미지 32,768개
- 텍스트 32,768개
- 비교: 32,768 × 32,768 ≈ 10억 쌍
```

### 4.3 정답과 오답

**정답**: 원래 인터넷에서 함께 있던 이미지-텍스트 쌍
**오답**: 다른 이미지의 텍스트와 임의로 조합한 것

```
배치 내 유사도 행렬:

        텍스트1  텍스트2  텍스트3  텍스트4
이미지1    ✓       ✗       ✗       ✗
이미지2    ✗       ✓       ✗       ✗
이미지3    ✗       ✗       ✓       ✗
이미지4    ✗       ✗       ✗       ✓

✓ = 정답 (대각선, 32,768개)
✗ = 오답 (나머지 약 10억 개)
```

### 4.4 학습 목표

**정답 쌍의 유사도가 가장 높도록** 인코더를 조정한다.

```
이미지1과 모든 텍스트의 유사도:

텍스트1 (정답): 0.95  ← 이게 제일 높아야 함!
텍스트2 (오답): 0.30
텍스트3 (오답): 0.15
텍스트4 (오답): 0.42
```

---

## 5. 유사도 계산

### 5.1 코사인 유사도 (Cosine Similarity)

두 벡터가 **얼마나 같은 방향**을 가리키는지 측정한다.

```
범위:
 1 = 완전히 같은 방향 (동일)
 0 = 직각 (관련 없음)
-1 = 정반대 방향
```

### 5.2 비슷한 개념은 높은 유사도

고양이 입벌림 사진과 "길고양이 눈감고 있다"는 오답 관계이지만, 둘 다 고양이이므로 유사도가 높다 (예: 0.85).

하지만 정답 쌍의 유사도가 더 높다 (예: 0.95).

**학습 목표: 정답이 1등이 되게 만드는 것** (다른 것들이 0점일 필요 없음)

---

## 6. 역전파 (Backpropagation)

### 6.1 핵심 개념

**벡터 자체를 직접 수정하는 것이 아니다.**
**인코더(기계) 내부의 파라미터를 수정한다.**

### 6.2 인코더 구조

인코더는 **수억 개의 파라미터(가중치)**로 이루어져 있다.

```
CLIP 파라미터 수:
- 이미지 인코더: 약 8,600만 개
- 텍스트 인코더: 약 6,300만 개
- 총합: 약 1억 5천만 개
```

### 6.3 역전파 과정

1. **순전파**: 이미지/텍스트 → 벡터 → 유사도 → 손실 계산
2. **역전파**: 손실에서 시작해 "각 파라미터의 책임"을 역방향으로 추적
3. **업데이트**: 책임(기울기)에 따라 파라미터를 조금씩 수정
4. **반복**: 배치마다 수백만 번

### 6.4 비유: 공장 불량품 추적

```
[원료] → [1공정] → [2공정] → [3공정] → [완제품: 불량!]

역방향으로 책임 추적:
- 3공정: "내 잘못 30%"
- 2공정: "내 잘못 50%"
- 1공정: "내 잘못 20%"

→ 각 공정을 책임만큼 수정
```

### 6.5 Learning Rate

한 번에 얼마나 수정할지 결정하는 값이다.

```
너무 크게 바꾸면: 발산 (망함)
조금씩 바꾸면: 수렴 (성공)
```

---

## 7. 학습 결과: 클러스터링

### 7.0 중요한 구분: 목표 vs 결과

**흔한 오해**: "CLIP은 클러스터링을 학습한다"

**정확한 이해**:
```
CLIP이 학습한 것:
  "이 이미지와 이 텍스트가 짝인지 아닌지 구분하기"
  (정답 쌍의 유사도↑, 오답 쌍의 유사도↓)

학습의 부산물 (의도하지 않았지만 자연스럽게 생긴 것):
  → 비슷한 것들이 가까이 모임 (클러스터링)
```

**클러스터링은 목표가 아니라 결과다.**

### 7.1 의미적으로 비슷한 것끼리 모임

정답 쌍을 가깝게, 오답 쌍을 멀게 학습하다 보면, 자연스럽게 512차원 공간에서 비슷한 개념들이 **클러스터**를 형성한다.

```
512차원 공간 (시각화):

    ●■ ●■        ← 고양이 클러스터
   ●■  ●■ ●■        (●=이미지, ■=텍스트)
     ●■●■

                ▲▼
              ▲▼ ▲▼  ← 강아지 클러스터
               ▲▼

★☆
★☆ ★☆              ← 비행기 클러스터
★☆
```

### 7.2 계층적 구조

```
동물 대클러스터
├── 고양이 클러스터
│   ├── 입 벌린 고양이들
│   ├── 자는 고양이들
│   └── 뛰는 고양이들
├── 강아지 클러스터
└── 새 클러스터

탈것 대클러스터
├── 비행기 클러스터
├── 자동차 클러스터
└── 배 클러스터
```

---

## 8. CLIP의 한계

### 8.1 데이터 편향

인터넷에 흔한 것은 잘 알고, 희귀한 것은 모른다.

| 카테고리 | 성능 |
|---------|------|
| 고양이, 강아지, 자동차 | 좋음 |
| 마크로파지, 희귀 곤충 | 나쁨 |
| 의료 영상, 위성 사진 | 나쁨 |

### 8.2 해결책: 전문 CLIP

| 모델 | 학습 데이터 | 잘하는 것 |
|------|------------|----------|
| CLIP (오리지널) | 인터넷 전체 | 일반 상식 |
| BiomedCLIP | 의학 논문 | 세포, 장기, 질병 |
| PubMedCLIP | PubMed 논문 | 의료 영상 |

### 8.3 False Negative 문제

의미적으로 정답인데 오답으로 취급되는 경우가 있다.

예: 텍스트가 완전히 동일한 두 쌍
```
쌍 a: [고양이 웅크림] + "우리집 고양이 웅크리고있다"
쌍 b: [뚱뚱한 고양이 웅크림] + "우리집 고양이 웅크리고있다"
```

교차해도 의미적으로 맞지만, CLIP은 오답으로 취급한다. 다행히 대규모 데이터에서는 큰 문제가 되지 않는다.

---

## 9. CLIP의 활용

### 9.1 텍스트로 이미지 검색

```
"해변에서 뛰는 강아지" → 벡터 → 가장 유사한 이미지 찾기
```

### 9.2 이미지 분류 (Zero-shot)

학습 없이도 텍스트만으로 분류 가능:
```
새 사진 → 벡터

"고양이" → 벡터 (거리: 0.02) ✓
"강아지" → 벡터 (거리: 0.55)

→ 고양이로 분류
```

### 9.3 뇌 연구 활용

```
사람이 이미지를 봄 → fMRI 측정 → CLIP 벡터 예측 → 원래 이미지 추론
```

---

*마지막 업데이트: 2026-01-07*
