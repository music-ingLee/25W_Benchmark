# 2025.01.07 학습 내용 정리

> CLIP, Foundation Model, Brain Foundation Model에 대한 학습 요약

---

## Part 1: 기초 개념

### 표상 (Representation)

```
무언가를 다른 형태로 "나타낸 것"
예: 고양이 → 뇌의 뉴런 활동 패턴
    고양이 사진 → CLIP의 512차원 벡터
```

### 피처 (Feature)

```
무언가를 설명하는 특징들의 숫자 목록
예: 바나나 → [0.9, 0.1, 0.8, ...] (길쭉함, 노란색 등)

중요: 각 숫자가 뭘 의미하는지 정확히 모름 (블랙박스)
     하지만 알아내려는 연구들 있음 (최대 활성화, probing 등)
```

### 512차원인 이유

```
특별한 이유 없음, 연구자가 정함
2의 거듭제곱 (2⁹) = 컴퓨터 계산에 효율적
적당한 크기 (너무 작으면 정보 부족, 너무 크면 비용 과다)
```

---

## Part 2: CLIP

### CLIP이란?

```
이미지와 텍스트를 "비교 가능하게" 만드는 AI 모델
둘 다 512차원 벡터로 변환 → 유사도 비교 가능
```

### 학습 과정

```
1. 데이터: 이미지-텍스트 쌍 4억 개 (인터넷에서 수집)
2. 배치 단위 학습 (32,768개씩)
3. 정답 쌍: 원래 붙어있던 것 → 가깝게
   오답 쌍: 다른 것 섞은 것 → 멀게
4. 유사도: 코사인 유사도 (방향 비교)
```

### 중요한 구분

```
CLIP이 학습한 것: "짝 맞추기" (정답 유사도↑, 오답 유사도↓)
학습의 부산물: 클러스터링 (비슷한 것끼리 모임)

→ 클러스터링은 목표가 아니라 결과!
```

### 한계

```
- 인터넷에 흔한 것 잘 알고, 희귀한 것(마크로파지 등) 모름
- 해결책: 전문 CLIP (BiomedCLIP 등)
```

---

## Part 3: 딥러닝 핵심 개념

### 역전파 (Backpropagation)

```
벡터를 직접 수정 ❌
인코더의 파라미터를 수정 ✓

과정:
1. 순전파: 입력 → 출력 → 손실 계산
2. 역전파: 손실에서 "각 파라미터 책임" 추적
3. 업데이트: 책임만큼 파라미터 조금씩 수정
4. 반복: 수백만 번
```

### 딥러닝 vs Ridge Regression

```
Ridge Regression:
- 1층, 선형
- 공식으로 한 방에 계산
- 역전파 필요 없음

딥러닝:
- 여러 층 (깊다 = deep)
- 비선형, 복잡한 패턴 학습 가능
- 역전파 필수
```

### Transformer

```
현대 AI의 핵심 구조 (GPT, BERT, CLIP 등)
핵심: Attention (어디에 주의를 기울일지 계산)
장점: 먼 거리도 직접 참조, 병렬 처리 가능
```

### CLIP 이미지 인코더 (ViT)

```
이미지 224×224 → 49개 패치로 자름
→ 각 패치를 768차원 벡터로
→ Transformer 12층 통과
→ 512차원 CLIP 벡터 출력

파라미터: 약 8,600만 개
```

---

## Part 4: Foundation Model

### 정의

```
대규모 데이터로 미리 학습해둔 "만능 기초 모델"
한 번 학습 → 여러 작업에 적용 가능
```

### Pre-training vs Fine-tuning

```
Pre-training (사전 학습):
- 대규모 데이터로 "일반 패턴" 학습
- Foundation Model 생성

Fine-tuning (미세 조정):
- 특정 작업에 맞게 추가 학습
- 적은 데이터로도 가능
```

### 중요한 깨달음

```
GPT가 법률, 의학, 코딩 다 잘하는 이유:
각 분야별 Fine-tuning ❌
Pre-training이 워낙 방대해서 이미 다 "읽었음" ✓

ChatGPT Fine-tuning = "친절하게 대화하는 법"만 추가

Zero-shot = Fine-tuning 없이 바로 새 작업 수행 가능
```

---

## Part 5: Brain Foundation Model

### 정의

```
대규모 뇌 데이터(fMRI)로 학습한 "뇌 해석 기초 모델"
```

### 학습 방식들

```
1. CLIP처럼 짝 맞추기: fMRI ↔ 이미지/CLIP 벡터
2. BERT처럼 가린 부분 예측
3. GPT처럼 다음 시점 예측
```

### 할 수 있는 작업들

```
- 뇌 디코딩: fMRI → 본 것/생각 추론
- 뇌 인코딩: 자극 → 뇌 반응 예측
- 질환 진단: fMRI → 알츠하이머 등 분류
- 개인차 분석: fMRI → 성격/IQ 예측
```

### CLIP과의 연결

```
뇌 활동 → CLIP 벡터 예측 → 이미지 찾기/재구성
왜? 512개만 예측하면 되니까 (픽셀 수백만 개보다 쉬움)
```

---

## Part 6: Horikawa (2020)의 한계 → Foundation Model 필요성

| Horikawa의 한계 | Foundation Model의 해결 |
|----------------|------------------------|
| 피험자 5명 | 수천 명 데이터 통합 |
| 개별 모델 | 공통 모델 + Fine-tuning |
| 특정 자극에 한정 | 다양한 자극으로 일반화 |
| 다른 연구와 단절 | 통합 기반 제공 |
| 선형 모델 (Ridge) | 비선형 딥러닝 가능 |

```
그럼에도 Horikawa 연구의 가치:
- 5명으로도 의미있는 결과
- 감정의 신경 표상 구조 밝힘
- Foundation Model의 "필요성"을 보여준 선구적 연구
```

---

## 오늘의 핵심 연결고리

```
CLIP (이미지-텍스트 매칭)
        ↓
Foundation Model 개념 이해
        ↓
Brain Foundation Model
        ↓
Horikawa 연구와의 비교
        ↓
신경과학 + AI의 융합 이해
```

---

## 관련 문서

- [CLIP 상세 정리](./CLIP_summary.md)
- [Foundation Model 상세 정리](./Foundation_Model_summary.md)

---

*작성일: 2025-01-07*
