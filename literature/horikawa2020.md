# Horikawa et al., 2020 논문 정리

## 기본 정보

| 항목 | 내용 |
|------|------|
| **제목** | The Neural Representation of Visually Evoked Emotion Is High-Dimensional, Categorical, and Distributed across Transmodal Brain Regions |
| **저자** | Tomoyasu Horikawa, Alan S. Cowen, Dacher Keltner, Yukiyasu Kamitani |
| **저널** | iScience (Cell Press) |
| **출판** | 2020년 5월 |
| **DOI** | 10.1016/j.isci.2020.101060 |

---

## 1. 연구 배경 및 동기

### 감정 연구의 핵심 논쟁

```
┌─────────────────────────────────────────────────────────────┐
│                    감정은 어떻게 표상되는가?                   │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   Basic Emotion Theory          Dimensional Theory          │
│   (기본 감정 이론)               (차원 이론)                  │
│                                                             │
│   "6개의 기본 감정이            "모든 감정은 2개 차원으로     │
│    별개로 존재한다"              설명된다"                    │
│                                                             │
│   예: 행복, 슬픔, 분노,          예: Valence (긍정-부정)      │
│       공포, 혐오, 놀람               Arousal (각성 수준)      │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 이 논문의 접근
- **34개 감정 카테고리** 사용 (기존 6개보다 훨씬 세분화)
- 카테고리 vs 차원 모델을 **직접 비교**

---

## 2. 실험 설계

### 자극 (Stimuli)
- **2,185개 감정 유발 비디오** (무음)
- 다양한 감정을 유발하도록 선정

### 피험자 (Participants)
- **5명**
- Whole-brain fMRI 측정

### 감정 레이블링
크라우드 워커들이 각 비디오에 대해 평가:

| 유형 | 개수 | 예시 |
|------|------|------|
| 감정 카테고리 | 34개 | joy, fear, disgust, amusement, awe, anxiety... |
| 정서 차원 | 14개 | valence, arousal, dominance, certainty... |

### 실험 구조
```
Training Set: 2,005개 비디오
Test Set: 180개 비디오 (따로 분리)

각 비디오 → fMRI 측정 → 감정 레이블과 연결
```

---

## 3. 분석 방법

### 3.1 Neural Decoding (디코딩)

```
뇌 활동 패턴  →  [모델]  →  감정 점수 예측

"이 뇌 활동을 보면, 어떤 감정인지 알 수 있는가?"
```

- **목적**: 뇌에 감정 정보가 있는지 확인
- **방법**: Ridge regression, Cross-validation

### 3.2 Voxel-wise Encoding (인코딩)

```
감정 점수  →  [모델]  →  개별 복셀 활동 예측

"특정 감정은 뇌의 어디에서 활성화되는가?"
```

- **목적**: 감정 정보가 뇌의 어디에 인코딩되는지 확인
- **방법**: 각 복셀에 대해 개별 회귀 모델

### 3.3 Unsupervised Modeling (비지도 학습)

```
뇌 활동 패턴들  →  [UMAP + k-means]  →  클러스터 구조 발견

"뇌 활동 패턴이 어떤 구조로 분포하는가?"
```

- **목적**: 레이블 없이 뇌 활동의 자연스러운 구조 탐색
- **방법**: UMAP 차원 축소 + k-means clustering

---

## 4. 주요 발견

### Finding 1: 분산 표상 (Distributed Representation)

```
❌ 과거 가정: 공포 → 편도체(amygdala)만 활성화
✅ 실제 결과: 공포 → 여러 영역 네트워크가 함께 활성화

감정은 특정 영역에 국한되지 않고,
여러 영역에 걸쳐 분산되어 표상됨
```

- 개인 간 일관성 있음 (5명 모두 유사한 패턴)

### Finding 2: 카테고리 > 차원

```
카테고리 모델 (34개 감정)  vs  차원 모델 (valence, arousal)
              ↓
      368/370 뇌 영역에서 카테고리 모델 승리!
```

- 심지어 amygdala에서도 카테고리 모델이 더 잘 예측
- **차원 모델의 한계**를 보여줌

### Finding 3: Transmodal 영역에 집중

```
처리 계층:
Visual (시각) → Semantic (의미) → Emotion (감정)
    ↓                ↓                 ↓
1차 시각피질    측두엽 영역      Transmodal 영역
                              (Default Mode Network)
```

- 감정 정보는 **고차 연합 영역**에서 주로 인코딩
- Principal Gradient 분석으로 확인

### Finding 4: 클러스터 + 연속 그라디언트

```
뇌 활동 패턴의 구조:

     [fear]----[horror]     ← 유사 감정은 가까움
        \        /
         \      /
          [awe]
           |
     [amusement]----[joy]   ← 유사 감정은 가까움
```

- 감정별로 **클러스터** 형성
- 관련된 감정 사이에는 **연속적 그라디언트**

---

## 5. 이론적 의의

1. **High-dimensional emotion space** 지지
   - 2차원(valence-arousal)보다 훨씬 복잡한 공간

2. **카테고리 접근** 지지
   - Basic emotion theory를 정교화
   - 6개가 아닌 34개로 확장

3. **분산 표상** 확인
   - 뇌 전체가 감정 처리에 참여
   - 특정 영역 = 특정 감정 (X)

---

## 6. 데이터 활용 아이디어

> **사수 질문**: 이 데이터를 어떻게 활용하면 좋을지?

### 아이디어 1: 다른 모델 적용
- 논문: Ridge regression 사용
- **제안**: Deep learning (CNN, Transformer) 적용
- **기대**: 비선형 관계 포착, 더 높은 예측 성능

### 아이디어 2: 개인차 분석
- 논문: 5명의 평균 패턴 분석
- **제안**: 개인별 차이 분석
- **기대**: 성격, 정서조절능력 등과 연결

### 아이디어 3: 감정 전이 분석
- 논문: 각 비디오를 독립적으로 분석
- **제안**: 비디오 시청 중 감정 변화 추적
- **기대**: 시간에 따른 감정 역학

### 아이디어 4: 크로스-모달 일반화
- 논문: 비디오(시각) 자극만 사용
- **제안**: 음악, 텍스트 등 다른 자극과 비교
- **기대**: 모달리티 독립적인 감정 표상 발견

---

## 7. 데이터 한계점

> **사수 질문**: 이 데이터가 갖는 한계점은?

### 한계 1: 적은 피험자 수
- **문제**: 5명만 측정
- **영향**: 일반화 어려움, 개인차 통계적 검정 제한
- **보완**: 더 많은 피험자 데이터 수집 또는 다른 데이터셋과 통합

### 한계 2: 생태학적 타당성
- **문제**: fMRI 스캐너 안에서 비디오 시청
- **영향**: 실제 감정 경험과 다를 수 있음
- **보완**: 경험 표집법(ESM)과 결합, 자연스러운 환경에서 측정

### 한계 3: 크라우드 소싱 레이블
- **문제**: 비디오 시청자가 느낀 감정 ≠ 피험자가 느낀 감정
- **영향**: 레이블 노이즈, 개인차 미반영
- **보완**: 피험자 본인의 감정 보고 수집

### 한계 4: 서양 중심 감정 카테고리
- **문제**: 34개 카테고리가 영어권 문화 기반
- **영향**: 문화 특수적 감정 누락
- **보완**: 다문화 감정 분류 체계 적용

### 한계 5: 정적 분석
- **문제**: 비디오 전체에 대해 하나의 fMRI 패턴
- **영향**: 시간에 따른 감정 변화 포착 불가
- **보완**: 시계열 분석, sliding window 적용

---

## 8. 후속 연구 방향

1. **메타 분석**: 다른 감정 fMRI 연구와 결과 통합
2. **임상 적용**: 정서 장애(우울, 불안) 환자의 패턴 비교
3. **실시간 fMRI**: neurofeedback을 통한 감정 조절 훈련
4. **멀티모달**: EEG, 피부전도 등 다른 측정과 결합

---

## 9. 논문 읽기 질문

> 논문 정독하면서 생기는 질문들을 여기에 기록

### 아직 이해 안 되는 부분
- [ ] Ridge regression이 정확히 어떻게 작동하는지?
- [ ] UMAP은 PCA와 뭐가 다른지?
- [ ] Principal Gradient가 무엇인지?
- [ ] Cross-validation의 fold 수는 어떻게 정했는지?

### 더 알고 싶은 부분
- [ ] 34개 카테고리는 어떻게 선정했는지?
- [ ] 다른 연구에서도 비슷한 결과가 나왔는지?
- [ ] 이 논문 이후 후속 연구는?

---

*작성일: 2026-01-03*
*상태: 초안 (논문 정독 후 업데이트 예정)*
