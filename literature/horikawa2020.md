# Horikawa et al., 2020 논문 정리

## 기본 정보

| 항목 | 내용 |
|------|------|
| **제목** | The Neural Representation of Visually Evoked Emotion Is High-Dimensional, Categorical, and Distributed across Transmodal Brain Regions |
| **저자** | Tomoyasu Horikawa, Alan S. Cowen, Dacher Keltner, Yukiyasu Kamitani |
| **저널** | iScience (Cell Press) |
| **출판** | 2020년 5월 |
| **DOI** | 10.1016/j.isci.2020.101060 |

---

## 1. 연구 배경 및 동기

### 감정 연구의 핵심 논쟁

```
┌─────────────────────────────────────────────────────────────┐
│                    감정은 어떻게 표상되는가?                   │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   Basic Emotion Theory          Dimensional Theory          │
│   (기본 감정 이론)               (차원 이론)                  │
│                                                             │
│   "6개의 기본 감정이            "모든 감정은 2개 차원으로     │
│    별개로 존재한다"              설명된다"                    │
│                                                             │
│   예: 행복, 슬픔, 분노,          예: Valence (긍정-부정)      │
│       공포, 혐오, 놀람               Arousal (각성 수준)      │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 이 논문의 접근
- **34개 감정 카테고리** 사용 (기존 6개보다 훨씬 세분화)
- 카테고리 vs 차원 모델을 **직접 비교**

---

## 2. 실험 설계

### 자극 (Stimuli)
- **2,185개 감정 유발 비디오** (무음)
- 다양한 감정을 유발하도록 선정

### 피험자 (Participants)
- **5명**
- Whole-brain fMRI 측정

### 감정 레이블링
크라우드 워커들이 각 비디오에 대해 평가:

| 유형 | 개수 | 예시 |
|------|------|------|
| 감정 카테고리 | 34개 | joy, fear, disgust, amusement, awe, anxiety... |
| 정서 차원 | 14개 | valence, arousal, dominance, certainty... |

### 실험 구조
```
Training Set: 2,005개 비디오
Test Set: 180개 비디오 (따로 분리)

각 비디오 → fMRI 측정 → 감정 레이블과 연결
```

---

## 3. 분석 방법

### 3.1 Neural Decoding (디코딩)

```
뇌 활동 패턴  →  [모델]  →  감정 점수 예측

"이 뇌 활동을 보면, 어떤 감정인지 알 수 있는가?"
```

- **목적**: 뇌에 감정 정보가 있는지 확인
- **방법**: Ridge regression, Cross-validation

### 3.2 Voxel-wise Encoding (인코딩)

```
감정 점수  →  [모델]  →  개별 복셀 활동 예측

"특정 감정은 뇌의 어디에서 활성화되는가?"
```

- **목적**: 감정 정보가 뇌의 어디에 인코딩되는지 확인
- **방법**: 각 복셀에 대해 개별 회귀 모델

### 3.3 Unsupervised Modeling (비지도 학습)

```
뇌 활동 패턴들  →  [UMAP + k-means]  →  클러스터 구조 발견

"뇌 활동 패턴이 어떤 구조로 분포하는가?"
```

- **목적**: 레이블 없이 뇌 활동의 자연스러운 구조 탐색
- **방법**: UMAP 차원 축소 + k-means clustering

---

## 4. 주요 발견

### Finding 1: 분산 표상 (Distributed Representation)

```
❌ 과거 가정: 공포 → 편도체(amygdala)만 활성화
✅ 실제 결과: 공포 → 여러 영역 네트워크가 함께 활성화

감정은 특정 영역에 국한되지 않고,
여러 영역에 걸쳐 분산되어 표상됨
```

- 개인 간 일관성 있음 (5명 모두 유사한 패턴)

### Finding 2: 카테고리 > 차원

```
카테고리 모델 (34개 감정)  vs  차원 모델 (valence, arousal)
              ↓
      368/370 뇌 영역에서 카테고리 모델 승리!
```

- 심지어 amygdala에서도 카테고리 모델이 더 잘 예측
- **차원 모델의 한계**를 보여줌

### Finding 3: Transmodal 영역에 집중

```
처리 계층:
Visual (시각) → Semantic (의미) → Emotion (감정)
    ↓                ↓                 ↓
1차 시각피질    측두엽 영역      Transmodal 영역
                              (Default Mode Network)
```

- 감정 정보는 **고차 연합 영역**에서 주로 인코딩
- Principal Gradient 분석으로 확인

### Finding 4: 클러스터 + 연속 그라디언트

```
뇌 활동 패턴의 구조:

     [fear]----[horror]     ← 유사 감정은 가까움
        \        /
         \      /
          [awe]
           |
     [amusement]----[joy]   ← 유사 감정은 가까움
```

- 감정별로 **클러스터** 형성
- 관련된 감정 사이에는 **연속적 그라디언트**

---

## 5. 이론적 의의

1. **High-dimensional emotion space** 지지
   - 2차원(valence-arousal)보다 훨씬 복잡한 공간

2. **카테고리 접근** 지지
   - Basic emotion theory를 정교화
   - 6개가 아닌 34개로 확장

3. **분산 표상** 확인
   - 뇌 전체가 감정 처리에 참여
   - 특정 영역 = 특정 감정 (X)

---

## 6. 데이터 활용 아이디어

> **사수 질문**: 이 데이터를 어떻게 활용하면 좋을지?

### 아이디어 1: 다른 모델 적용
- 논문: Ridge regression 사용
- **제안**: Deep learning (CNN, Transformer) 적용
- **기대**: 비선형 관계 포착, 더 높은 예측 성능

### 아이디어 2: 개인차 분석
- 논문: 5명의 평균 패턴 분석
- **제안**: 개인별 차이 분석
- **기대**: 성격, 정서조절능력 등과 연결

### 아이디어 3: 감정 전이 분석
- 논문: 각 비디오를 독립적으로 분석
- **제안**: 비디오 시청 중 감정 변화 추적
- **기대**: 시간에 따른 감정 역학

### 아이디어 4: 크로스-모달 일반화
- 논문: 비디오(시각) 자극만 사용
- **제안**: 음악, 텍스트 등 다른 자극과 비교
- **기대**: 모달리티 독립적인 감정 표상 발견

---

## 7. 데이터 한계점

> **사수 질문**: 이 데이터가 갖는 한계점은?

### 한계 1: 적은 피험자 수
- **문제**: 5명만 측정
- **영향**: 일반화 어려움, 개인차 통계적 검정 제한
- **보완**: 더 많은 피험자 데이터 수집 또는 다른 데이터셋과 통합

### 한계 2: 생태학적 타당성
- **문제**: fMRI 스캐너 안에서 비디오 시청
- **영향**: 실제 감정 경험과 다를 수 있음
- **보완**: 경험 표집법(ESM)과 결합, 자연스러운 환경에서 측정

### 한계 3: 크라우드 소싱 레이블
- **문제**: 비디오 시청자가 느낀 감정 ≠ 피험자가 느낀 감정
- **영향**: 레이블 노이즈, 개인차 미반영
- **보완**: 피험자 본인의 감정 보고 수집

### 한계 4: 서양 중심 감정 카테고리
- **문제**: 34개 카테고리가 영어권 문화 기반
- **영향**: 문화 특수적 감정 누락
- **보완**: 다문화 감정 분류 체계 적용

### 한계 5: 정적 분석
- **문제**: 비디오 전체에 대해 하나의 fMRI 패턴
- **영향**: 시간에 따른 감정 변화 포착 불가
- **보완**: 시계열 분석, sliding window 적용

---

## 8. 후속 연구 방향

1. **메타 분석**: 다른 감정 fMRI 연구와 결과 통합
2. **임상 적용**: 정서 장애(우울, 불안) 환자의 패턴 비교
3. **실시간 fMRI**: neurofeedback을 통한 감정 조절 훈련
4. **멀티모달**: EEG, 피부전도 등 다른 측정과 결합

---

## 9. Figure별 상세 분석

### Figure 1: 실험 개요
```
┌─────────────────────────────────────────────────────────────┐
│  2185개 감정 비디오 (무음)                                    │
│         ↓                                                   │
│  피험자 5명이 시청 (whole-brain fMRI 측정)                    │
│         ↓                                                   │
│  3가지 분석 방법:                                            │
│  • Neural Decoding: 뇌 → 감정 예측                          │
│  • Voxel-wise Encoding: 감정 → 뇌 활동 예측                  │
│  • Unsupervised Modeling: 클러스터 구조 발견                 │
└─────────────────────────────────────────────────────────────┘
```

**Figure 1B 핵심**: 34개 카테고리의 점수 분포
- 각 비디오마다 여러 rater가 감정 평가
- 카테고리: 0~1 비율 (해당 감정을 느낀 비율)
- 차원: 1~9 Likert scale

---

### Figure 2: Neural Decoding 결과

**핵심 발견**:
1. **31/34개 카테고리**가 뇌에서 유의미하게 디코딩됨
   - 실패: envy, contempt, guilt (비디오에서 잘 유발 안 됨)

2. **Ensemble decoder**가 개별 영역보다 성능 좋음
   - 카테고리: 84.1% 향상
   - 차원: 74.3% 향상

3. **감정별로 다른 뇌 영역 패턴**
   - fear ≠ horror (유사해 보여도 다른 영역 사용)
   - 개인 간 일관성 높음 (5명 모두 유사)

**Figure 2G**: 감정 식별 정확도
- 5-way identification: 71.4% (카테고리), 59.3% (차원)
- Chance level: 20%

---

### Figure 3: 비디오 식별 분석

**핵심**: 디코딩된 감정 점수로 2181개 비디오 중 어떤 걸 봤는지 맞출 수 있나?

**결과**:
- 카테고리 모델: **81.9%** 정확도
- 차원 모델: **68.7%** 정확도
- Chance: 50%

**Figure 3D**: UMAP 시각화
- 34차원 감정 공간 → 2차원으로 축소
- 디코딩된 점수로 재구성한 맵이 원본과 유사
- 상관계수: r = 0.46 (x축), r = 0.53 (y축)

---

### Figure 4: Voxel-wise Encoding 결과

**핵심 질문**: 감정 점수로 각 복셀의 활동을 예측할 수 있는가?

**결과**:
- **368/370 영역**에서 카테고리 모델 > 차원 모델
- **Amygdala에서도** 카테고리가 더 잘 예측!
  - 전통적으로 amygdala = valence/arousal이라고 생각했으나...
  - 실제로는 카테고리 정보가 더 많이 인코딩됨

**Figure 4D**: Slope angle 분석
- 45도보다 작으면 → 카테고리 승
- 거의 모든 영역이 45도 미만

---

### Figure 5: 감정 vs 시각 vs 의미 특징 분리

**핵심 질문**: 뇌가 감정을 인코딩하는가, 아니면 시각/의미 특징을 인코딩하는가?

**3가지 모델 비교**:
1. **Visual object model**: VGG 딥러닝 네트워크 출력
2. **Semantic model**: 크라우드 소싱 의미 태그 (예: cats, indoor)
3. **Emotion model**: 34개 감정 카테고리 점수

**결과 (Figure 5E)**:
- **Visual cortex**: 시각 모델 승
- **Temporal/Parietal**: 의미 모델 승
- **Transmodal regions (DMN)**: 감정 모델 승!

**Figure 5F-H: Principal Gradient 분석**
```
Unimodal ←──────────────────────────→ Transmodal
(1차 감각)                              (고차 연합)
   │                                        │
   Visual        Semantic            Emotion
   model         model               model
   best          best                best
```

---

### Figure 6: Unsupervised Modeling

**핵심**: 레이블 없이 뇌 활동 패턴만으로 클러스터가 형성되는가?

**방법**:
1. 감정 관련 복셀만 선택
2. UMAP으로 2D 시각화
3. k-means clustering (k=27)

**결과 (Figure 6A)**:
- 뇌 활동 패턴이 **감정 카테고리별 클러스터** 형성
- Sexual desire, Disgust, Aesthetic appreciation 등 명확한 클러스터
- Valence, Arousal로 색칠하면 클러스터 불분명

**Figure 6D-E: Entropy 분석**
- 카테고리: 낮은 entropy (특정 클러스터에 집중)
- 차원: 높은 entropy (여러 클러스터에 분산)

---

## 10. 핵심 방법론 상세

### Ridge Regression (정규화 선형 회귀)

```
일반 선형 회귀:  min ||y - Xw||²
Ridge 회귀:      min ||y - Xw||² + λ||w||²
                                   ↑
                            정규화 항 (과적합 방지)
```

**왜 필요한가?**
- fMRI 데이터: 복셀 수(수만 개) >> 샘플 수(2181개)
- 과적합 위험 높음
- λ가 가중치 크기 제한 → 일반화 성능 향상

### UMAP vs PCA

| 특성 | PCA | UMAP |
|------|-----|------|
| 방식 | 선형 변환 | 비선형 manifold learning |
| 보존 | 전역 분산 | 지역 구조 + 전역 구조 |
| 시각화 | 클러스터 불명확 | 클러스터 명확 |
| 속도 | 빠름 | 중간 |

### Cross-Validation (6-fold)

```
데이터를 6등분:
[1][2][3][4][5][6]

Round 1: [1]이 test, [2-6]이 train
Round 2: [2]가 test, [1,3-6]이 train
...
Round 6: [6]이 test, [1-5]가 train

→ 모든 데이터가 한 번씩 test로 사용됨
→ 과적합 방지 + 일반화 성능 추정
```

---

## 11. 논문의 Limitations (저자들이 언급)

1. **감정 레이블이 피험자 본인 것이 아님**
   - 크라우드 워커 평가 ≠ fMRI 피험자 경험
   - 개인차 반영 어려움

2. **1인칭 vs 3인칭 감정 혼재**
   - 본인이 느끼는 감정 vs 타인 감정 추론
   - Theory of Mind 영역과 겹침

3. **시선 추적 없음**
   - 감정에 따라 시선 패턴이 다를 수 있음
   - 이게 뇌 활동에 영향 줄 수 있음

4. **반복 측정 없음**
   - 각 비디오 1회만 제시
   - noise ceiling 추정 불가

---

## 12. 데이터/코드 접근

**공개 자료**:
- GitHub: https://github.com/KamitaniLab/EmotionVideoNeuralRepresentation
- OpenNeuro: https://openneuro.org/datasets/ds002425
- Mendeley Data: https://doi.org/10.17632/jbk2r73mzh.1

---

## 13. 논문 읽기 질문

> 논문 정독하면서 생기는 질문들을 여기에 기록

### 아직 이해 안 되는 부분
- [ ] Ridge regression이 정확히 어떻게 작동하는지
- [ ] UMAP은 PCA와 뭐가 다른지
- [ ] Principal Gradient가 무엇인지
- [ ] Cross-validation의 fold 수는 어떻게 정했는지

### 더 알고 싶은 부분
- [ ] 34개 카테고리는 어떻게 선정했는지?
- [ ] VGG 네트워크의 어떤 레이어를 사용했는지?
- [ ] HCP360 parcellation의 상세 정보
- [ ] 다른 연구에서도 비슷한 결과가 나왔는지?

### 나의 질문/생각
> (논문 읽으면서 직접 채우기)
-
-
-

---

*작성일: 2026-01-03*
*최종 업데이트: 2026-01-03*
*상태: AI가 작성한 프리뷰 - 본인 학습 필요*

**Note**: 이 문서는 AI가 논문을 읽고 정리한 것입니다.
실제로 이해했는지는 직접 논문을 읽고 확인해야 합니다.
