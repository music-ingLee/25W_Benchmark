# Horikawa et al. (2020) iScience 정리노트

> **논문 제목**: The Neural Representation of Visually Evoked Emotion Is High-Dimensional, Categorical, and Distributed across Transmodal Brain Regions
> **저널**: iScience (Cell Press), 2020
> **저자**: Tomoyasu Horikawa, Alan S. Cowen, Dacher Keltner, Yukiyasu Kamitani

---

## 목차

1. [왜 이 연구가 필요했는가? - Cowen 2017의 한계](#1-왜-이-연구가-필요했는가---cowen-2017의-한계)
2. [핵심 연구 질문](#2-핵심-연구-질문)
3. [실험 설계](#3-실험-설계)
4. [3가지 분석 접근법](#4-3가지-분석-접근법)
5. [주요 발견](#5-주요-발견)
6. [핵심 방법론 상세](#6-핵심-방법론-상세)
7. [Figure별 심층 분석](#7-figure별-심층-분석)
8. [이론적 함의](#8-이론적-함의)
9. [한계점](#9-한계점)
10. [Cowen 2017과의 종합](#10-cowen-2017과의-종합)

---

## 1. 왜 이 연구가 필요했는가? - Cowen 2017의 한계

### 1.1 Cowen & Keltner (2017)이 보여준 것

Cowen 2017은 **self-report**(주관적 보고)에서 감정의 구조를 탐구했다:

```
결론: 감정 경험은 카테고리적이지만, 연속적 그라디언트로 연결됨
      (27개 카테고리, 차원 모델보다 설명력 높음)
```

### 1.2 Cowen 2017이 답하지 못한 질문

| 질문 | Cowen 2017로 답할 수 있는가? |
|:----|:------------------------|
| 사람들이 감정을 **보고할 때** 카테고리가 더 유용한가? | ✓ Yes |
| 감정이 **뇌에서** 카테고리적으로 표상되는가? | ❌ **모름** |
| 감정 **경험 자체**가 카테고리적인가? | ❌ **모름** |

> **핵심 한계**: Self-report는 **언어**로 매개된다.
> 사람들이 "fear"라고 보고하는 이유가:
> (A) 뇌에서 fear가 별개 상태로 존재해서? 아니면
> (B) "fear"라는 단어가 있어서 그렇게 라벨링하는 것?

### 1.3 Horikawa 2020의 역할

```
Cowen 2017:    비디오 → self-report → 카테고리 > 차원
                            ↑
                       언어로 매개됨 (언어적 습관일 수도?)

Horikawa 2020: 비디오 → fMRI 뇌 활동 → 카테고리 > 차원?
                            ↑
                       언어 없이 직접 측정
```

**만약** 뇌 활동에서도 같은 패턴이 나온다면 → self-report의 발견이 단순히 "언어적 습관"이 아니라 **신경 표상 수준에서도 유효**하다는 더 강한 주장이 가능

### 1.4 저자 구성의 의미

```
Horikawa (신경과학)  +  Cowen & Keltner (감정 심리학)  +  Kamitani (디코딩 전문가)
     ↓                        ↓                              ↓
  fMRI 실험            34개 카테고리 프레임워크           Brain Decoding 기술
                       + 2185개 비디오 자극
```

**같은 자극, 같은 감정 분류 체계**를 사용해서 self-report 연구와 직접 비교 가능!

---

## 2. 핵심 연구 질문

> **"감정(emotion)의 신경 표상(neural representation)은 어떤 구조인가?"**

구체적으로 세 가지 하위 질문:

### 질문 1: 저차원인가, 고차원인가?

**"차원"의 의미**: 감정을 설명하는 데 필요한 **변수의 개수**

```
저차원 (Low-dimensional) = Dimensional Theory
┌──────────────────────────────────────────────┐
│  "2개 축이면 모든 감정을 설명할 수 있다"        │
│                                              │
│       arousal (+)                            │
│           ↑                                  │
│           │   fear●    ●anger               │
│           │                                  │
│  valence(-) ←──┼──→ valence(+)              │
│           │                                  │
│           │   sadness●  ●calm               │
│           ↓                                  │
│       arousal (-)                            │
│                                              │
│  fear = (valence: -0.8, arousal: +0.9)       │
│  → 2개 숫자로 위치 지정                        │
└──────────────────────────────────────────────┘

고차원 (High-dimensional) = Categorical Theory
┌──────────────────────────────────────────────┐
│  "아니, 2개론 부족해. 34개 카테고리가 필요해"   │
│                                              │
│  fear ≠ anger (둘 다 negative + high arousal │
│                 인데도 분명히 다르잖아!)       │
│                                              │
│  fear = [fear: 1.0, anger: 0.2, joy: 0.0,    │
│          awe: 0.1, disgust: 0.3, ...]        │
│  → 34개 숫자로 위치 지정                       │
└──────────────────────────────────────────────┘
```

**핵심**: Dimensional Theory는 저차원을, Categorical Theory는 고차원을 가정함

### 질문 2: 카테고리적인가, 연속적인가?

```
카테고리적 (Categorical)
┌─────────────────┐
│ [fear] [joy]    │
│    ●     ●      │  ← 별개의 클러스터
│ [anger] [awe]   │
│    ●     ●      │
└─────────────────┘

연속적 (Continuous)
┌─────────────────┐
│                 │
│    ●  ●  ●  ●   │  ← 경계 없이 섞여 있음
│  ●  ●  ●  ●  ●  │
│                 │
└─────────────────┘
```

### 질문 3: 국소적인가, 분산적인가?

```
국소적 (Localized)              분산적 (Distributed)
┌─────────────────┐            ┌─────────────────┐
│                 │            │  ●      ●       │
│  fear → 편도체   │            │    ●  ●  ●     │
│  joy → 측좌핵    │            │  ●    ●   ●    │
│  (1:1 매핑)     │            │ (네트워크 전체)  │
└─────────────────┘            └─────────────────┘
```

---

## 3. 실험 설계

### 3.1 자극: Cowen 2017과 동일

```
총 비디오:  2,185개
형식:       무음 비디오 (5-10초)
내용:       다양한 감정 유발 장면

Cowen 2017에서 수집한 레이블 그대로 사용:
- 34개 감정 카테고리 (0~1 비율)
- 14개 정서 차원 (1~9 Likert)
```

### 3.2 피험자

| 항목 | 내용 |
|:-----|:-----|
| 인원 | 5명 (Sub1~Sub5) |
| 측정 | Whole-brain fMRI |
| 세션 | 여러 세션에 걸쳐 모든 비디오 시청 |

**왜 5명인가?**
- fMRI는 비용과 시간 소모 큼
- 대신 **피험자 내 반복 측정** 없이 **많은 자극**(2185개) 사용
- 개인 간 일관성으로 일반화 가능성 확인

### 3.3 데이터 분할

```
총 2,185개 비디오
         ↓
┌────────────────────────────────┐
│  Training Set: 2,005개         │  ← 모델 학습용
├────────────────────────────────┤
│  Test Set: 180개               │  ← 성능 평가용
│  (따로 분리, 학습에 사용 안 함)  │
└────────────────────────────────┘
```

### 3.4 fMRI 전처리

```
Raw fMRI data
     ↓
Motion correction (움직임 보정)
     ↓
Slice timing correction
     ↓
Normalization to MNI space
     ↓
Spatial smoothing
     ↓
HCP360 Parcellation (360개 영역으로 분할)
     ↓
각 영역의 평균 활동 패턴
```

---

## 4. 3가지 분석 접근법

### 4.1 Neural Decoding (디코딩)

```
┌─────────────────────────────────────────────────────────┐
│                    Neural Decoding                       │
├─────────────────────────────────────────────────────────┤
│                                                         │
│    뇌 활동 패턴  →  [Ridge Regression]  →  감정 점수     │
│    (input)              (model)           (output)       │
│                                                         │
│    "이 뇌 활동을 보면, 어떤 감정인지 알 수 있는가?"        │
│                                                         │
└─────────────────────────────────────────────────────────┘

질문: 뇌에 감정 정보가 있는가?
답:   디코딩이 성공하면 → Yes, 정보가 있음
```

**Cowen 2017의 예측과 비교**:
- 차원 → 카테고리 예측 61% (Cowen, self-report)
- 뇌 → 카테고리 예측 ??% (Horikawa, fMRI)

### 4.2 Voxel-wise Encoding (인코딩)

```
┌─────────────────────────────────────────────────────────┐
│                   Voxel-wise Encoding                    │
├─────────────────────────────────────────────────────────┤
│                                                         │
│    감정 점수  →  [Ridge Regression]  →  복셀 활동        │
│    (input)          (각 복셀마다)        (output)        │
│                                                         │
│    "감정이 뇌의 어디에 어떻게 인코딩되는가?"              │
│                                                         │
└─────────────────────────────────────────────────────────┘

질문: 카테고리 모델 vs 차원 모델, 어느 것이 뇌를 더 잘 설명하는가?
```

**핵심 비교**:
```
모델 A (카테고리, 고차원):  34개 감정 점수 → 복셀 활동 예측
모델 B (차원, 저차원):      14개 차원 점수 → 복셀 활동 예측
                              ↓
                   어느 모델이 더 정확한가?
```

### 4.3 Unsupervised Modeling (비지도 학습)

```
┌─────────────────────────────────────────────────────────┐
│                  Unsupervised Modeling                   │
├─────────────────────────────────────────────────────────┤
│                                                         │
│    뇌 활동 패턴들  →  [UMAP + k-means]  →  클러스터       │
│    (레이블 없이)                          (자연적 구조)   │
│                                                         │
│    "레이블 없이 뇌 활동만 보면 어떤 구조가 나타나는가?"    │
│                                                         │
└─────────────────────────────────────────────────────────┘

질문: 뇌 활동의 "자연스러운" 구조는 카테고리에 가까운가, 차원에 가까운가?
```

---

## 5. 주요 발견

### Finding 1: 감정은 뇌에서 디코딩 가능하다

```
34개 카테고리 중 31개가 유의미하게 디코딩됨!

성공 (31개): fear, joy, awe, disgust, horror, amusement, sadness,
            anger, excitement, romance, ... 등

실패 (3개):  envy, contempt, guilt
            → 이 감정들은 비디오로 잘 유발되지 않음
```

**디코딩 성능**:
| 지표 | 카테고리 모델 (고차원) | 차원 모델 (저차원) |
|:----|:-------------------|:----------------|
| 5-way identification | **71.4%** | 59.3% |
| Chance level | 20% | 20% |

> **해석**: 뇌에 감정 정보가 분명히 있고, 고차원 카테고리 프레임워크로 더 잘 읽힌다

### Finding 2: 카테고리 > 차원 (거의 모든 영역에서)

```
370개 뇌 영역에서 카테고리 vs 차원 모델 비교:

카테고리(고차원) 승: 368개 영역 (99.5%)
차원(저차원) 승:      2개 영역  (0.5%)

심지어 amygdala에서도 카테고리 모델 승!
```

**왜 이게 놀라운가?**

기존 상식:
```
"Amygdala는 valence(좋고 나쁨)를 처리한다"
→ 저차원 모델이 잘 맞을 것으로 예상
```

실제 결과:
```
Amygdala에서도 카테고리 모델이 더 잘 예측
→ "valence 탐지기"보다 "감정 유형 탐지기"에 가까움
```

### Finding 3: 감정 표상은 Transmodal 영역에 집중

```
뇌의 기능적 계층 (Principal Gradient):

Unimodal ←────────────────────────────────→ Transmodal
(1차 감각)                                   (고차 연합)
    │                                            │
    │     Visual      Semantic      Emotion      │
    │     model       model         model        │
    │     best        best          best         │
    │       ↓           ↓             ↓          │
    V1, V2, V4    MT, LO, STS    DMN, mPFC, TPJ  │
```

**비유**: 감정 처리의 "위계"

```
1층 (Visual):    "움직이는 물체가 있다"
2층 (Semantic):  "그게 뱀이다"
3층 (Emotion):   "무섭다!"
```

### Finding 4: 분산 표상 (Distributed Representation)

```
❌ 과거 가정:
fear → 편도체
joy → 측좌핵
disgust → 섬엽
(1:1 매핑)

✅ 실제 발견:
fear → 편도체 + mPFC + ACC + 섬엽 + ... (네트워크)
joy → 측좌핵 + vmPFC + PCC + ... (네트워크)
disgust → 섬엽 + ACC + mPFC + ... (네트워크)
```

> 각 감정은 **특정 영역이 아닌 영역들의 조합(패턴)**으로 표상됨

### Finding 5: 클러스터 + 연속적 그라디언트

```
비지도 학습으로 발견한 뇌 활동의 구조:

       [Sexual desire]          [Aesthetic appreciation]
              ●                        ●
             / \                      / \
    [Romance]   [Craving]    [Awe]      [Admiration]
        ●          ●          ●            ●
                    \        /
                     [Horror]
                        ●
                       / \
              [Fear]      [Disgust]
                ●            ●
```

- **클러스터**: 감정별로 뚜렷한 군집 형성
- **연속성**: 관련 감정들 사이에 그라디언트 존재

**Cowen 2017 결과와 동일한 패턴!**

---

## 6. 핵심 방법론 상세

### 6.1 Ridge Regression

**왜 일반 회귀가 아닌 Ridge인가?**

```
문제: fMRI 데이터의 "차원의 저주"

복셀 수:    수만 개 (독립변수)
샘플 수:    2,005개 (비디오)
           ↓
        복셀 >> 샘플 → 과적합 위험!
```

**Ridge의 해결책**:

```
일반 회귀:  min ||y - Xw||²
            "예측 오차만 최소화"

Ridge:      min ||y - Xw||² + λ||w||²
                             ↑
                      "가중치도 작게 유지"
                      → 과적합 방지
```

**비유**:
```
일반 회귀 = 시험 문제만 외우기 (과적합)
Ridge    = 원리를 이해하기 (일반화)
```

### 6.2 UMAP vs PCA

| 특성 | PCA | UMAP |
|:-----|:----|:-----|
| **방식** | 선형 변환 | 비선형 manifold learning |
| **보존** | 전역 분산 | 지역 구조 + 전역 구조 |
| **클러스터** | 불명확 | 명확 |
| **해석** | 축이 의미 가짐 | 축 자체는 의미 없음 |

**왜 UMAP을 선택?**
- 감정의 클러스터 구조를 시각화하기 위해
- 고차원(34개 감정)을 2D로 효과적으로 압축

### 6.3 Encoding vs Decoding

```
┌────────────────────────────────────────────────────────┐
│                      Decoding                          │
│  뇌 → 감정                                             │
│  "뇌에 감정 정보가 있는가?" (존재 여부)                  │
│  전체 뇌를 하나의 입력으로 사용                         │
└────────────────────────────────────────────────────────┘

┌────────────────────────────────────────────────────────┐
│                      Encoding                          │
│  감정 → 뇌                                             │
│  "감정이 뇌의 어디에 인코딩되는가?" (위치)              │
│  각 복셀/영역별로 개별 모델                             │
└────────────────────────────────────────────────────────┘
```

**둘 다 필요한 이유**:
- Decoding: 정보의 존재 확인 (Yes/No)
- Encoding: 정보의 분포 확인 (Where/How)

### 6.4 Cross-Validation (6-fold)

```
데이터를 6등분:
[1][2][3][4][5][6]

Round 1: [1]이 test, [2-6]이 train
Round 2: [2]가 test, [1,3-6]이 train
...
Round 6: [6]이 test, [1-5]가 train

→ 모든 데이터가 한 번씩 test로 사용됨
→ 과적합 방지 + 일반화 성능 추정
```

---

## 7. Figure별 심층 분석

### Figure 1: 실험 개요

```
┌─────────────────────────────────────────────────────────┐
│  A. 자극과 레이블                                        │
│                                                         │
│  2185개 비디오 → 크라우드 워커 평정                      │
│                     ↓                                   │
│              34 카테고리 + 14 차원                       │
│                                                         │
│  B. fMRI 측정                                           │
│                                                         │
│  피험자 5명 × 2185개 비디오 시청                         │
│         ↓                                               │
│  Whole-brain BOLD signal                                │
│                                                         │
│  C. 3가지 분석                                          │
│                                                         │
│  1. Decoding: 뇌 → 감정                                 │
│  2. Encoding: 감정 → 뇌                                 │
│  3. Unsupervised: 클러스터 구조                          │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Figure 2: Neural Decoding 결과

**핵심 메시지**: 뇌에서 감정 정보를 읽을 수 있다

```
Figure 2A-C: 개별 영역 디코딩
- 각 HCP360 영역에서 별도 디코딩
- 영역마다 디코딩 정확도 다름
- 어떤 영역도 혼자서 완벽하지 않음

Figure 2D-E: Ensemble 디코딩
- 여러 영역 조합 시 성능 향상
- 카테고리: 84.1% 향상
- 차원: 74.3% 향상
→ 감정은 분산 표상

Figure 2F-G: 피험자 간 일관성
- 5명 모두 유사한 패턴
- 개인차보다 공통성이 큼
```

### Figure 3: 비디오 식별

**핵심 질문**: 디코딩된 감정 점수로 어떤 비디오를 봤는지 맞출 수 있는가?

```
방법:
1. 뇌 활동 → 감정 점수 디코딩
2. 디코딩된 점수 vs 실제 레이블 비교
3. 2181개 중 가장 유사한 비디오 찾기

결과:
┌───────────────────────┬──────────────┐
│ 카테고리 모델 (고차원)  │    81.9%     │
│ 차원 모델 (저차원)      │    68.7%     │
│ Chance                │    50%       │
└───────────────────────┴──────────────┘

→ 고차원 카테고리 모델이 더 정확하게 비디오 식별
```

### Figure 4: Voxel-wise Encoding

**핵심 메시지**: 거의 모든 뇌 영역에서 카테고리 모델이 우세

```
Figure 4A-C: 영역별 encoding 정확도
- 각 영역에서 카테고리 vs 차원 모델 비교
- 빨간색: 카테고리 승
- 파란색: 차원 승

Figure 4D: Slope angle 분석
        카테고리 정확도
             ↑
             │    ●
             │  ● ● ●
             │● ● ● ● ●
        ─────┼─────────→ 차원 정확도
             │

점이 대각선 아래 = 카테고리 승
점이 대각선 위 = 차원 승

결과: 368/370 영역이 대각선 아래!
```

### Figure 5: 시각 vs 의미 vs 감정

**핵심 질문**: 뇌가 감정을 인코딩하는가, 아니면 시각/의미 특징을 인코딩하는가?

```
세 가지 competing 모델:

1. Visual model:   VGG 딥러닝 출력 (이미지 특징)
2. Semantic model: 크라우드 소싱 의미 태그 (cats, indoor, etc.)
3. Emotion model:  34개 감정 점수

결과 (영역별 승자):
┌────────────────────────────────────────────────────┐
│  Visual cortex (V1-V4)  →  Visual model 승         │
│  Temporal/Parietal      →  Semantic model 승       │
│  Transmodal (DMN)       →  Emotion model 승!       │
└────────────────────────────────────────────────────┘

→ 감정 정보는 고차 연합 영역에서 주로 표상
```

### Figure 6: Unsupervised Clustering

**핵심 메시지**: 레이블 없이도 뇌 활동이 감정별 클러스터를 형성

```
방법:
1. 감정 관련 복셀만 선택 (encoding 결과 기반)
2. UMAP으로 2D 시각화
3. k-means clustering (k=27)

결과 시각화:
┌─────────────────────────────────────────────────┐
│                                                 │
│  ○ Sexual desire        ○ Aesthetic appreciation│
│       ○ Romance    ○ Amusement                  │
│                    ○ Joy                        │
│                                                 │
│  ○ Horror                                       │
│       ○ Fear   ○ Disgust                        │
│                                                 │
│  ○ Sadness                                      │
│       ○ Empathic pain                           │
│                                                 │
└─────────────────────────────────────────────────┘

Entropy 분석:
- 카테고리로 색칠: 낮은 entropy (클러스터 명확)
- 차원으로 색칠: 높은 entropy (클러스터 불분명)
```

---

## 8. 이론적 함의

### 8.1 감정 이론에 대한 시사점

| 이론 | 차원수 | Horikawa 결과 |
|:-----|:------|:-------------|
| **Basic Emotion** (Ekman) | 저~중차원 (6개) | △ 카테고리 구조는 있지만 6개보다 많음 |
| **Dimensional** (Russell) | 저차원 (2개) | ✗ 차원 모델이 카테고리에 패배 |
| **Constructionist** | 가변적 | △ 분산 표상과 부분 일치 |
| **High-dimensional categorical** (Cowen) | 고차원 (27~34개) | ✓ 가장 잘 맞음 |

### 8.2 왜 고차원 카테고리인가?

```
진화적 관점:
"다양한 상황에 맞는 다양한 반응이 필요했다"

  위협 상황     → fear (도망)
  오염물 접촉   → disgust (회피)
  사회적 위반   → anger (대응)
  성적 기회     → desire (접근)
  ...

2차원(좋다/나쁘다, 흥분/이완)으로는 이런 구분이 불가능
→ 고차원 카테고리 표상이 적응적
```

### 8.3 분산 표상의 의미

```
왜 감정이 "한 곳"에 있지 않을까?

감정 = 지각 + 기억 + 신체 상태 + 행동 준비 + 사회적 판단 + ...
        ↓
     여러 기능의 통합
        ↓
     여러 영역의 협업 필요
```

---

## 9. 한계점

### 9.1 저자들이 인정한 한계 (논문 p.16-17)

1. **감정 레이블이 피험자 본인 것이 아님**
   ```
   크라우드 워커의 평정 ≠ fMRI 피험자의 실제 경험
   → 개인차 반영 어려움 (문화, 인구통계, 성격)
   ```

2. **1인칭 vs 3인칭 감정 혼재**
   ```
   비디오: 타인이 감정 표현하는 장면
   피험자: 본인이 느끼는 감정? 타인 감정 추론?

   감정 모델이 우세한 영역 (IPL, VMPFC)
   = Theory of Mind 네트워크와 중첩
   → 두 과정 분리 불가능
   ```

3. **시선 추적 없음** (free viewing)
   ```
   자유 시청 조건 → 감정마다 다른 시선 패턴?
   → 시선 패턴이 뇌 데이터에 영향 가능
   (단, 시선 표상은 특정 영역에 국소화)
   ```

4. **단일 자극 제시** (논문 p.15-16, Discussion)
   ```
   각 비디오 1회만 제시 → noise ceiling 추정 불가

   Why 반복 안 함?
   → surprise 같은 감정은 반복 시 변함
   → 감정 특성상 반복 측정 부적절
   ```

### 9.2 추가 고려할 한계 (논문에 명시되지 않음)

5. **피험자 수 적음 (5명)**
   ```
   개인차 통계 검정 제한
   일반화 주의 필요
   (완화: 2,181개 자극으로 피험자 내 변산성 확보)
   ```

6. **생태학적 타당성**
   ```
   fMRI 스캐너 안에서 비디오 시청
   ≠ 실제 삶에서의 감정 경험
   ```

7. **시간 해상도**
   ```
   fMRI는 느림 (TR = 2초)
   빠른 감정 변화 포착 어려움
   ```

8. **문화적 제한**
   ```
   34개 카테고리 = 영어권 감정 분류
   다른 문화의 감정 개념 누락 가능
   ```

9. **자극 양식 제한**
   ```
   시각 자극만 사용 (청각 없음)
   실제 감정 경험은 다중 감각적
   ```

> **상세 내용**: `horikawa2020_discussion_limitations.md` 참조

---

## 10. Cowen 2017과의 종합

### 10.1 두 연구의 수렴

```
┌─────────────────────────────────────────────────────────┐
│                    Cowen 2017                           │
│                                                         │
│  데이터:     Self-report (주관적 보고)                   │
│  방법:      회귀, 예측 비교                              │
│  결론:      고차원 카테고리 > 저차원 차원                 │
│            + 연속적 그라디언트                           │
│                                                         │
│                        ↓                                │
│           같은 자극, 같은 분류 체계                       │
│                        ↓                                │
│                                                         │
│                   Horikawa 2020                         │
│                                                         │
│  데이터:     fMRI (뇌 활동)                              │
│  방법:      Decoding, Encoding, Clustering              │
│  결론:      고차원 카테고리 > 저차원 차원                 │
│            + 연속적 그라디언트                           │
│                                                         │
└─────────────────────────────────────────────────────────┘

             ↓

두 수준에서 동일한 패턴!
Self-report ←→ Neural representation
```

### 10.2 이것이 의미하는 바

```
가설 A: "카테고리 구조는 언어적 습관일 뿐"
        → Horikawa 결과로 기각됨
        → 언어 없이 측정한 뇌에서도 같은 구조

가설 B: "감정의 카테고리 구조는 신경 수준에서 실재"
        → Horikawa 결과와 일치
        → Self-report가 뇌 표상을 반영
```

### 10.3 남은 질문들

1. **인과 방향**: 뇌의 카테고리 구조 → 언어? 언어 → 뇌?
2. **발달**: 이 구조는 선천적? 학습된?
3. **문화 보편성**: 다른 문화권에서도 같은 구조?
4. **임상 적용**: 정서 장애에서 이 구조가 변형되는가?

---

## 참고 자료

- **GitHub**: https://github.com/KamitaniLab/EmotionVideoNeuralRepresentation
- **OpenNeuro**: https://openneuro.org/datasets/ds002425
- **Cowen Interactive Map**: https://s3-us-west-1.amazonaws.com/emogifs/map.html

---

## 더 공부할 것

- [ ] Ridge regression의 λ 선택 방법
- [ ] UMAP 알고리즘 상세
- [ ] Principal Gradient (Margulies et al., 2016)
- [ ] HCP360 parcellation의 영역 이름과 기능
- [ ] Default Mode Network의 역할

---

*마지막 업데이트: 2026-01-04*
*Cowen 2017 노트와 연계하여 작성*
*한계점 섹션 검증 및 수정 완료*
